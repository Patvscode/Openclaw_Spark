# 2026-02-12

## Rate Limit Optimization (Telegram group: "Solving rate limits" id:-5115563017)
- Hit 30k input tokens/min limit on Opus (Anthropic API)
- Deleted BOOTSTRAP.md (no longer needed)
- Trimmed all workspace files: AGENTS.md (~4KB‚Üí870B), SOUL.md, USER.md, IDENTITY.md, TOOLS.md
- Saved ~3-4K tokens per turn

## GitHub Backup
- Repo: Patvscode/Openclaw_Spark (PAT stored in git remote URL)
- ‚ö†Ô∏è Pat shared PAT in group chat msg ‚Äî should rotate token
- Daily auto-backup cron at noon EST (cron id: d023e831) ‚Äî runs via Bob, zero Opus cost
- .gitignore excludes heartbeat-state.json, logs, .env

## Bob Autonomous Worker System
- Cron every 30 min (cron id: 6d0a9598) ‚Äî Bob wakes, picks task, writes findings, adds follow-ups
- Task queue: /home/pmello/.openclaw/workspace-bob/tasks/QUEUE.md
- Findings dir: /home/pmello/.openclaw/workspace-bob/findings/
- Instructions: /home/pmello/.openclaw/workspace-bob/BOB_INSTRUCTIONS.md
- 15 min timeout per session, ollama/gpt-oss:120b-ctx131k model
- Seeded 3 starter tasks: self-improving AI research, 6-DOF arm survey, two-brain architectures

## Bob Performance Baseline
- Test task (3 subtasks): 2 min 14 sec, 14.1K tokens in, 405 out, $0.00
- Good for: research, file ops, summaries, backups, non-time-sensitive work
- Not for: real-time chat, time-sensitive responses

## Mini Agent (20B MXFP4)
- Downloaded gpt-oss:20b (~13GB) ‚Äî MXFP4 native quantization optimized for DGX Spark GB10
- Registered as agent "mini" in OpenClaw config
- Test: 18 seconds for math + file listing (vs Bob's 2m14s)
- Shares workspace-bob directory

## Agent Hierarchy
- üß† Spark (Opus) ‚Äî orchestrator, complex reasoning, Pat's interface
- ü§ñ Bob (120B, ~65GB) ‚Äî deep research, ~2 min/task
- ‚ö° Mini (20B MXFP4, ~13GB) ‚Äî quick tasks, ~18s/task

## Concurrency Config
- maxConcurrent: 2 agents, 3 sub-agents
- Memory math: 128GB total, ~10GB OS, 65GB Bob, 13GB/Mini
- With Bob loaded: room for ~4 Minis. Without Bob: ~9 Minis
- Ollama serializes inference on single GPU ‚Äî parallel slots needed for true parallelism

## Parallel Inference (PENDING ‚Äî needs Pat's sudo)
- Need OLLAMA_NUM_PARALLEL=3 in ollama.service
- Command given to Pat: `sudo bash -c 'sed -i "/\[Service\]/a Environment=\"OLLAMA_NUM_PARALLEL=3\"" /lib/systemd/system/ollama.service && systemctl daemon-reload && systemctl restart ollama'`
- This enables 3 concurrent Mini inference slots via batched GPU inference

## Bob's Cron Jobs
| Cron ID | Name | Schedule | Model |
|---------|------|----------|-------|
| d023e831 | workspace-git-backup | noon EST daily | 120B |
| 6d0a9598 | bob-autonomous-worker | every 30 min | 120B |
| 596ed0da | bob-memory-maintenance | 3 AM daily | 120B |
| b31dc776 | bob-findings-organizer | 6 AM daily | 120B |

## Key Decisions
- All Telegram bindings go through Spark (main/Opus), Bob only via sub-agent spawn
- Compaction mode: safeguard (default, fine)
- No heartbeat configured ‚Äî saves Opus budget
- Pat wants Bob to self-direct: research, test, refine his own schedule overnight
- Future: fine-tuning 20B models for specialized tasks once enough training data from Bob's findings
- Future: consider vLLM or llama.cpp server for better parallel throughput
