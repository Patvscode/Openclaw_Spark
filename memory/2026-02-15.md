# 2026-02-15 — Day 4: Ultimate Team Goes Live

## Major Breakthrough: Tool Calling WORKS
- CODER (qwen3-coder) successfully made structured tool_calls through OpenClaw → Ollama
- It called web_search AND web_fetch correctly — got real results, fetched 4 URLs in parallel
- The fix: models needed tool name hints in SOUL.md (they guessed `run` instead of `exec`, `write_file` instead of `write`)
- This unblocks ALL agent autonomy

## DeepSeek R1 Cannot Use Tools
- deepseek-r1:14b returns "does not support tools" — hard rejection from Ollama
- Swapped REASON agent from deepseek-r1:14b to qwen3:14b
- DeepSeek R1 stays in library for standalone reasoning tasks only (no agent use)

## Ultimate Team Deployed
- Old structure (Bob + 8 minis) fully disabled — 12 cron jobs turned off
- New agents: PRIME (qwen3:32b), FLASH (qwen3:30b-a3b), CODER (qwen3-coder), REASON (qwen3:14b), RUNNER (qwen3:14b), VISION (qwen3-vl:8b)
- Bob kept in reserve (not loaded by default)
- Flash (qwen3:30b-a3b) downloaded — 18GB MoE model, 256K context
- Total team VRAM: ~82GB, 46GB headroom

## First Research Tasks — Results
- **FLASH**: Researched AI-Scientist v2 — search + fetch + findings in 81 seconds. Confirmed v2 defaults to closed models but is patchable for local use (which we already did). Didn't write file but returned quality findings in response.
- **RUNNER**: Wrote research-ollama-tool-calling.md — tools worked (search + write) but content was hallucinated (fake benchmarks, said "avoid Qwen3" while being Qwen3). 14B too small for reliable factual research.
- **PRIME**: Ran exec successfully (full system inventory) but timed out before writing the report file. 32B thinks deeply but slow on multi-step.
- **CODER**: Used XML format for web_search (wrong), proper format for web_fetch (right). Fetched 4 URLs successfully but search calls failed. Added format fix to SOUL.md.

## Agent Tool-Use Capability Matrix (Tested)
| Agent | Model | exec | write | web_search | web_fetch | Verdict |
|-------|-------|------|-------|-----------|-----------|---------|
| FLASH | qwen3:30b-a3b | ✅ | ✅ | ✅ | ✅ | **MVP — perfect, fastest** |
| CODER | qwen3-coder | — | — | ⚠️ XML bug | ✅ | Needs format fix |
| PRIME | qwen3:32b | ✅ | ❌ timeout | — | — | Good exec, slow multi-step |
| RUNNER | qwen3:14b | — | ✅ | ✅ | — | Works, low quality content |
| VISION | qwen3-vl:8b | untested | — | — | — | — |
| REASON | qwen3:14b | untested | — | — | — | — |

## Key Lesson: Gateway Restarts Kill Running Tasks
- Config.patch triggers SIGUSR1 restart which aborts all active subagent sessions
- NEVER patch config while agents are running tasks
- First batch of research tasks was killed by the REASON model swap restart

## Config Changes
- agents.list completely replaced with new team
- agentToAgent.allow updated for new agent IDs
- Telegram bindings simplified to just main + bob
- Old mini Telegram accounts (alpha/beta/gamma/delta) still in config but unbound

## AI-Scientist Setup Complete
- PyTorch cu130 installed in AI-Scientist venv — CUDA working on GB10!
- Shakespeare char dataset prepped for NanoGPT template
- All deps installed (aider-chat, matplotlib, tiktoken, wandb, etc.)
- Ollama integration confirmed: create_client('ollama/qwen3:8b') works
- First test run launched: 52 ideas generated, executing 2 experiments
- Using qwen3:8b as research brain — fully local, zero API cost
- DGX Spark setup guide found: natolambert/dgx-spark-setup (invaluable for cu130+aarch64)
- sm_121 warning is safe to ignore (confirmed binary compatible with sm_120)
- VISION agent confirmed online — exec, web_search, write all working

## AI-Scientist First Successful Run
- Switched from qwen3:8b → qwen3-coder for AI-Scientist (no thinking mode = direct responses)
- **Critical discovery**: Qwen3 models (30b-a3b, 32b, 14b, 8b) have THINKING MODE ON by default
  - Via OpenAI-compatible endpoint: thinking goes to `reasoning` field, `content` comes back EMPTY
  - Thinking tokens count toward max_tokens limit — 4096 max_tokens gets eaten by thinking
  - qwen3-coder does NOT have thinking mode — perfect for tool/API use
  - To disable thinking via native Ollama API: `"think": false` at top level
  - Created flash-nothink modelfile but custom template didn't properly suppress thinking
- First experiment results (Adaptive Block Size vs Baseline):
  - Val loss: 1.78 vs 1.47 (worse) but train time: 532s vs 893s (40% faster!)
  - qwen3-coder successfully: planned experiments → wrote code diffs → debugged errors → ran GPU training
  - Run 2 actively training with improved variant
- Pipeline: ideas (3 generated) → aider codes changes → GPU runs → error feedback → debug → iterate
- AI-Scientist bugs fixed: KeyError 'novel' (added .get()), final_info filename mismatch (glob fallback), baseline_results parsing (flat vs nested)
- Baseline NanoGPT: 10.65M params, shakespeare_char, 5000 iters, ~15min on GB10

## Kimi K2.5
- 1 TRILLION params, 600GB disk — way too big for DGX Spark
- Ollama only has cloud version, no local download
- Top-rated open-source Feb 2026 but impractical for us

## Cron Jobs Active
- flash-research-loop: runs 9am/1pm/5pm, uses qwen3:30b-a3b for research topics
- daily-git-backup: noon, uses qwen3:30b-a3b (errored on first run — timeout)
- bob-memory-maintenance: 3am (uses gpt-oss:120b — won't work if Bob not loaded)
- bob-findings-organizer: 6am (same issue)
- workspace-git-backup: noon via Bob (worked)

## AI-Scientist Pipeline Hardened
- Created systemd service: `ai-scientist.service` (auto-restart, 60s delay, max 3/10min)
- Wrapper script: `~/.openclaw/workspace/ai-scientist-runner.sh`
- Logs dir: `~/.openclaw/workspace/ai-scientist-logs/` (status.json + timestamped run logs)
- Monitoring cron: every 30 min, announces to Telegram
- Root cause of previous death: nohup child killed when OpenClaw exec session cleaned up
- Fix: proper systemd service fully detached from OpenClaw process tree

## Key Insight: Tool Name Aliases
- Qwen3 models CAN do tool calling — they just used wrong names
- After getting "tool not found" errors, CODER adapted and used correct names
- Adding tool name hints to SOUL.md should fix this permanently
- This was THE blocker we've been stuck on for 2 days
